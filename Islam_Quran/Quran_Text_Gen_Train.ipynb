{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename,'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_tokens_to_sequences(corpus,sequence_length=50):\n",
    "    tokens = ' '.join(corpus)\n",
    "    tokens = tokens.split()\n",
    "    # organize into sequences of tokens\n",
    "    length = sequence_length + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 156893\n"
     ]
    }
   ],
   "source": [
    "# saving quranament sequences to txt file\n",
    "quran_sequences = corpus_to_tokens_to_sequences(clean_quran)\n",
    "out_filename = 'quran_sequences.txt'\n",
    "save_doc(quran_sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quran Text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            329000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6580)              664580    \n",
      "=================================================================\n",
      "Total params: 1,144,480\n",
      "Trainable params: 1,144,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "156893/156893 [==============================] - 155s 988us/sample - loss: 6.0406 - acc: 0.0624\n",
      "Epoch 2/50\n",
      "156893/156893 [==============================] - 152s 969us/sample - loss: 5.6176 - acc: 0.0926\n",
      "Epoch 3/50\n",
      "156893/156893 [==============================] - 152s 967us/sample - loss: 5.3224 - acc: 0.1229\n",
      "Epoch 4/50\n",
      "156893/156893 [==============================] - 167s 1ms/sample - loss: 5.1135 - acc: 0.1352\n",
      "Epoch 5/50\n",
      "156893/156893 [==============================] - 159s 1ms/sample - loss: 4.9489 - acc: 0.1446\n",
      "Epoch 6/50\n",
      "156893/156893 [==============================] - 173s 1ms/sample - loss: 4.8099 - acc: 0.1535\n",
      "Epoch 7/50\n",
      "156893/156893 [==============================] - 197s 1ms/sample - loss: 4.6852 - acc: 0.1629\n",
      "Epoch 8/50\n",
      "156893/156893 [==============================] - 173s 1ms/sample - loss: 4.5718 - acc: 0.1721\n",
      "Epoch 9/50\n",
      "156893/156893 [==============================] - 154s 982us/sample - loss: 4.4696 - acc: 0.1798\n",
      "Epoch 10/50\n",
      "156893/156893 [==============================] - 200s 1ms/sample - loss: 4.3750 - acc: 0.1868\n",
      "Epoch 11/50\n",
      "156893/156893 [==============================] - 223s 1ms/sample - loss: 4.2887 - acc: 0.1919\n",
      "Epoch 12/50\n",
      "156893/156893 [==============================] - 283s 2ms/sample - loss: 4.2075 - acc: 0.1978\n",
      "Epoch 13/50\n",
      "156893/156893 [==============================] - 151s 963us/sample - loss: 4.1318 - acc: 0.2037\n",
      "Epoch 14/50\n",
      "156893/156893 [==============================] - 152s 971us/sample - loss: 4.0682 - acc: 0.2086\n",
      "Epoch 15/50\n",
      "156893/156893 [==============================] - 160s 1ms/sample - loss: 3.9978 - acc: 0.2141\n",
      "Epoch 16/50\n",
      "156893/156893 [==============================] - 161s 1ms/sample - loss: 3.9365 - acc: 0.2197\n",
      "Epoch 17/50\n",
      "156893/156893 [==============================] - 163s 1ms/sample - loss: 3.8776 - acc: 0.2254\n",
      "Epoch 18/50\n",
      "156893/156893 [==============================] - 169s 1ms/sample - loss: 3.8221 - acc: 0.2310\n",
      "Epoch 19/50\n",
      "156893/156893 [==============================] - 168s 1ms/sample - loss: 3.7703 - acc: 0.2367\n",
      "Epoch 20/50\n",
      "156893/156893 [==============================] - 165s 1ms/sample - loss: 3.7229 - acc: 0.2413\n",
      "Epoch 21/50\n",
      "156893/156893 [==============================] - 163s 1ms/sample - loss: 3.6759 - acc: 0.2482\n",
      "Epoch 22/50\n",
      "156893/156893 [==============================] - 168s 1ms/sample - loss: 3.6300 - acc: 0.2528\n",
      "Epoch 23/50\n",
      "156893/156893 [==============================] - 165s 1ms/sample - loss: 3.5878 - acc: 0.2576\n",
      "Epoch 24/50\n",
      "156893/156893 [==============================] - 167s 1ms/sample - loss: 3.5462 - acc: 0.2629\n",
      "Epoch 25/50\n",
      "156893/156893 [==============================] - 166s 1ms/sample - loss: 3.5048 - acc: 0.2676\n",
      "Epoch 26/50\n",
      "156893/156893 [==============================] - 165s 1ms/sample - loss: 3.4675 - acc: 0.2724\n",
      "Epoch 27/50\n",
      "156893/156893 [==============================] - 160s 1ms/sample - loss: 3.4288 - acc: 0.2781\n",
      "Epoch 28/50\n",
      "156893/156893 [==============================] - 159s 1ms/sample - loss: 3.3950 - acc: 0.2817\n",
      "Epoch 29/50\n",
      "156893/156893 [==============================] - 157s 998us/sample - loss: 3.3580 - acc: 0.2868\n",
      "Epoch 30/50\n",
      "156893/156893 [==============================] - 172s 1ms/sample - loss: 3.3236 - acc: 0.2926\n",
      "Epoch 31/50\n",
      "156893/156893 [==============================] - 169s 1ms/sample - loss: 3.2890 - acc: 0.2966\n",
      "Epoch 32/50\n",
      "156893/156893 [==============================] - 166s 1ms/sample - loss: 3.2557 - acc: 0.3016\n",
      "Epoch 33/50\n",
      "156893/156893 [==============================] - 161s 1ms/sample - loss: 3.2202 - acc: 0.3065\n",
      "Epoch 34/50\n",
      "156893/156893 [==============================] - 158s 1ms/sample - loss: 3.1905 - acc: 0.3106\n",
      "Epoch 35/50\n",
      "156893/156893 [==============================] - 161s 1ms/sample - loss: 3.1583 - acc: 0.3153\n",
      "Epoch 36/50\n",
      "156893/156893 [==============================] - 169s 1ms/sample - loss: 3.1255 - acc: 0.3219\n",
      "Epoch 37/50\n",
      "156893/156893 [==============================] - 165s 1ms/sample - loss: 3.0947 - acc: 0.3261\n",
      "Epoch 38/50\n",
      "156893/156893 [==============================] - 161s 1ms/sample - loss: 3.0650 - acc: 0.3308\n",
      "Epoch 39/50\n",
      "156893/156893 [==============================] - 163s 1ms/sample - loss: 3.0369 - acc: 0.3352\n",
      "Epoch 40/50\n",
      "156893/156893 [==============================] - 163s 1ms/sample - loss: 3.0059 - acc: 0.3389\n",
      "Epoch 41/50\n",
      "156893/156893 [==============================] - 158s 1ms/sample - loss: 2.9772 - acc: 0.3442\n",
      "Epoch 42/50\n",
      "156893/156893 [==============================] - 152s 967us/sample - loss: 2.9488 - acc: 0.3488\n",
      "Epoch 43/50\n",
      "156893/156893 [==============================] - 154s 982us/sample - loss: 2.9213 - acc: 0.3528\n",
      "Epoch 44/50\n",
      "156893/156893 [==============================] - 154s 980us/sample - loss: 2.8954 - acc: 0.3576\n",
      "Epoch 45/50\n",
      "156893/156893 [==============================] - 156s 996us/sample - loss: 2.8661 - acc: 0.3631\n",
      "Epoch 46/50\n",
      "156893/156893 [==============================] - 155s 987us/sample - loss: 2.8401 - acc: 0.3670\n",
      "Epoch 47/50\n",
      "156893/156893 [==============================] - 159s 1ms/sample - loss: 2.8130 - acc: 0.3710\n",
      "Epoch 48/50\n",
      "156893/156893 [==============================] - 155s 991us/sample - loss: 2.7885 - acc: 0.3763\n",
      "Epoch 49/50\n",
      "156893/156893 [==============================] - 155s 985us/sample - loss: 2.7638 - acc: 0.3804\n",
      "Epoch 50/50\n",
      "156893/156893 [==============================] - 157s 1000us/sample - loss: 2.7396 - acc: 0.3838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a40aae7f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(quran_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(quran_sequences)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    " \n",
    "# define model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_length),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model to file\n",
    "model.save('quran_model_3.hdf5')\n",
    "# save the tokenizer\n",
    "pickle.dump(tokenizer, open('quran_tokenizer_3.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
